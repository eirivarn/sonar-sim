{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3642cafb",
   "metadata": {},
   "source": [
    "# Net Localization (Cartesian-first)\n",
    "\n",
    "Load sonar frames, convert to Cartesian, and extract net structure (edges/lines) for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a055216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Setup (uncomment if using Colab)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # Update the path to where you uploaded your data\n",
    "# COLAB_MODE = True\n",
    "# COLAB_DATA_PATH = '/content/drive/MyDrive/sonar-sim/simulation/data/runs/net_following_fish'\n",
    "\n",
    "# For local execution\n",
    "COLAB_MODE = False\n",
    "COLAB_DATA_PATH = None\n",
    "\n",
    "print(f\"Colab mode: {COLAB_MODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6922f6",
   "metadata": {},
   "source": [
    "## 0. Setup for Google Colab (Optional)\n",
    "\n",
    "If running on Google Colab, uncomment and run this cell first to mount your Google Drive and set the correct path to your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eb555d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK; inline extractor ready (Canny + Hough | LSD | Threshold+Skeleton)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from ipywidgets import interact, IntSlider, fixed\n",
    "from scipy import ndimage\n",
    "import sys\n",
    "\n",
    "# Basic evidence extraction helpers (inline, no external module)\n",
    "def make_config():\n",
    "    return {\n",
    "        \"denoise\": {\"d\": 5, \"sigmaColor\": 50, \"sigmaSpace\": 50},\n",
    "        \"clahe\": {\"clipLimit\": 2.0, \"tileGridSize\": (8, 8)},\n",
    "        \"canny\": {\"low\": 100, \"high\": 120},\n",
    "        \"hough\": {\"rho\": 1, \"theta\": np.pi / 180, \"threshold\": 50, \"minLineLength\": 40, \"maxLineGap\": 4},\n",
    "        \"lsd\": {\"scale\": 0.8, \"sigma_scale\": 0.6, \"quant\": 2.0, \"ang_th\": 22.5, \"log_eps\": 0, \"density_th\": 0.7, \"n_bins\": 1024},\n",
    "        \"threshold\": {\"value\": 80, \"maxval\": 255, \"type\": cv2.THRESH_BINARY},  # Direct intensity threshold\n",
    "        \"morph\": {\"kernel_size\": 3, \"iterations\": 1},  # Morphological operations\n",
    "    }\n",
    "\n",
    "CFG = make_config()\n",
    "\n",
    "# Steps\n",
    "\n",
    "def denoise(img_u8):\n",
    "    return cv2.bilateralFilter(img_u8, d=CFG[\"denoise\"][\"d\"], sigmaColor=CFG[\"denoise\"][\"sigmaColor\"], sigmaSpace=CFG[\"denoise\"][\"sigmaSpace\"])\n",
    "\n",
    "\n",
    "def enhance(img_u8):\n",
    "    clahe = cv2.createCLAHE(clipLimit=CFG[\"clahe\"][\"clipLimit\"], tileGridSize=CFG[\"clahe\"][\"tileGridSize\"])\n",
    "    return clahe.apply(img_u8)\n",
    "\n",
    "\n",
    "def edges(img_u8):\n",
    "    return cv2.Canny(img_u8, CFG[\"canny\"][\"low\"], CFG[\"canny\"][\"high\"])\n",
    "\n",
    "\n",
    "def lines(edges_img):\n",
    "    result = cv2.HoughLinesP(\n",
    "        edges_img,\n",
    "        rho=CFG[\"hough\"][\"rho\"],\n",
    "        theta=CFG[\"hough\"][\"theta\"],\n",
    "        threshold=CFG[\"hough\"][\"threshold\"],\n",
    "        minLineLength=CFG[\"hough\"][\"minLineLength\"],\n",
    "        maxLineGap=CFG[\"hough\"][\"maxLineGap\"],\n",
    "    )\n",
    "    return result if result is not None else []\n",
    "\n",
    "\n",
    "# ALTERNATIVE METHODS (no edge detection needed)\n",
    "\n",
    "def lines_lsd(img_u8):\n",
    "    \"\"\"Line Segment Detector - detects lines directly without edge detection\"\"\"\n",
    "    lsd = cv2.createLineSegmentDetector(\n",
    "        cv2.LSD_REFINE_STD,\n",
    "        scale=CFG[\"lsd\"][\"scale\"],\n",
    "        sigma_scale=CFG[\"lsd\"][\"sigma_scale\"],\n",
    "        quant=CFG[\"lsd\"][\"quant\"],\n",
    "        ang_th=CFG[\"lsd\"][\"ang_th\"],\n",
    "        log_eps=CFG[\"lsd\"][\"log_eps\"],\n",
    "        density_th=CFG[\"lsd\"][\"density_th\"],\n",
    "        n_bins=CFG[\"lsd\"][\"n_bins\"]\n",
    "    )\n",
    "    lines_result = lsd.detect(img_u8)[0]\n",
    "    \n",
    "    # Convert to HoughLinesP format: [[x1, y1, x2, y2]]\n",
    "    if lines_result is not None:\n",
    "        return lines_result.reshape(-1, 1, 4).astype(np.int32)\n",
    "    return []\n",
    "\n",
    "\n",
    "def morphological_skeleton(binary):\n",
    "    \"\"\"Simple morphological skeleton using erosion (scipy-based)\"\"\"\n",
    "    skeleton = np.zeros_like(binary, dtype=bool)\n",
    "    element = np.array([[0, 1, 0],\n",
    "                        [1, 1, 1],\n",
    "                        [0, 1, 0]], dtype=bool)\n",
    "    \n",
    "    eroded = binary.astype(bool)\n",
    "    while np.any(eroded):\n",
    "        opened = ndimage.binary_opening(eroded, structure=element)\n",
    "        skeleton |= eroded & ~opened\n",
    "        eroded = ndimage.binary_erosion(eroded, structure=element)\n",
    "    \n",
    "    return skeleton\n",
    "\n",
    "\n",
    "def threshold_skeleton(img_u8):\n",
    "    \"\"\"Threshold + morphological skeleton to extract line structure\"\"\"\n",
    "    # Apply threshold to keep only bright pixels (likely net)\n",
    "    _, binary = cv2.threshold(img_u8, CFG[\"threshold\"][\"value\"], \n",
    "                              CFG[\"threshold\"][\"maxval\"], \n",
    "                              CFG[\"threshold\"][\"type\"])\n",
    "    \n",
    "    # Morphological closing to connect nearby regions\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, \n",
    "                                       (CFG[\"morph\"][\"kernel_size\"], \n",
    "                                        CFG[\"morph\"][\"kernel_size\"]))\n",
    "    closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel, \n",
    "                              iterations=CFG[\"morph\"][\"iterations\"])\n",
    "    \n",
    "    # Extract skeleton using scipy\n",
    "    binary_bool = closed > 0\n",
    "    skeleton_bool = morphological_skeleton(binary_bool)\n",
    "    skeleton = (skeleton_bool * 255).astype(np.uint8)\n",
    "    \n",
    "    return skeleton\n",
    "\n",
    "\n",
    "def gradient_magnitude(img_u8):\n",
    "    \"\"\"Calculate gradient magnitude to highlight intensity changes\"\"\"\n",
    "    # Compute gradients using Sobel\n",
    "    grad_x = cv2.Sobel(img_u8, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    grad_y = cv2.Sobel(img_u8, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    \n",
    "    # Magnitude\n",
    "    magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "    magnitude = np.clip(magnitude, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return magnitude\n",
    "\n",
    "print(\"Imports OK; inline extractor ready (Canny + Hough | LSD | Threshold+Skeleton)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc4adef",
   "metadata": {},
   "source": [
    "## 1. Load frames and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e6ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 frames\n",
      "Example shape: None\n",
      "No frames loaded - check that the path exists and contains frame_*.npz files\n"
     ]
    }
   ],
   "source": [
    "# Paths and frame selection\n",
    "if COLAB_MODE and COLAB_DATA_PATH:\n",
    "    run_path = Path(COLAB_DATA_PATH)\n",
    "else:\n",
    "    run_path = Path.cwd().parent / 'simulation' / 'data' / 'runs' / 'net_following_fish'\n",
    "\n",
    "frames_dir = run_path / 'frames'\n",
    "frame_files = sorted(frames_dir.glob('frame_*.npz'))[:1000]\n",
    "\n",
    "sonar_images = []\n",
    "fov_list = []\n",
    "range_list = []\n",
    "meta_list = []\n",
    "\n",
    "for f in frame_files:\n",
    "    data = np.load(f, mmap_mode='r')\n",
    "    if 'sonar_image' not in data:\n",
    "        data.close(); continue\n",
    "    sonar_images.append(np.array(data['sonar_image']))\n",
    "    fov_deg = 120.0\n",
    "    range_m = 20.0\n",
    "    meta = {}\n",
    "    if 'meta_json' in data:\n",
    "        meta = json.loads(str(data['meta_json']))\n",
    "        fov_deg = float(meta.get('fov_deg', fov_deg))\n",
    "        range_m = float(meta.get('range_m', range_m))\n",
    "    fov_list.append(fov_deg)\n",
    "    range_list.append(range_m)\n",
    "    meta_list.append(meta)\n",
    "    data.close()\n",
    "\n",
    "print(f'Loaded {len(sonar_images)} frames')\n",
    "print(f'Example shape: {sonar_images[0].shape if sonar_images else None}')\n",
    "print(f'FOV range: {min(fov_list):.1f}–{max(fov_list):.1f} deg, range: {min(range_list):.1f}–{max(range_list):.1f} m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e070ad",
   "metadata": {},
   "source": [
    "## 2. Polar → Cartesian conversion helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8f3db1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polar_to_cartesian(polar_image, max_range_m=20.0, fov_deg=120.0, output_size=400):\n",
    "    r_bins, n_beams = polar_image.shape\n",
    "    half_width = max_range_m * np.sin(np.radians(fov_deg / 2))\n",
    "    x = np.linspace(-half_width, half_width, output_size)\n",
    "    y = np.linspace(0, max_range_m, output_size)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    R = np.sqrt(X**2 + Y**2)\n",
    "    Theta = np.arctan2(X, Y)\n",
    "    fov_rad = np.radians(fov_deg)\n",
    "    beam_idx = (Theta + fov_rad / 2) / fov_rad * (n_beams - 1)\n",
    "    range_idx = (R / max_range_m) * (r_bins - 1)\n",
    "    beam_idx = np.clip(beam_idx, 0, n_beams - 1).astype(np.float32)\n",
    "    range_idx = np.clip(range_idx, 0, r_bins - 1).astype(np.float32)\n",
    "    cart = cv2.remap(polar_image.astype(np.float32), beam_idx, range_idx, cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n",
    "    cart[(R > max_range_m) | (Theta < -fov_rad/2) | (Theta > fov_rad/2)] = 0\n",
    "    extent = [-half_width, half_width, 0, max_range_m]\n",
    "    return cart, extent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3b5ff2",
   "metadata": {},
   "source": [
    "## 3. Visualize raw sonar (polar & Cartesian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f584cb8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TraitError",
     "evalue": "setting max < min",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTraitError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2018614823.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0minteract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIntSlider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msonar_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Frame'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipywidgets/widgets/widget_int.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value, min, max, step, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0m__init__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_bounded_int_doc_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipywidgets/widgets/widget_int.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value, min, max, step, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_BoundedInt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipywidgets/widgets/widget_int.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipywidgets/widgets/widget.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;34m\"\"\"Public constructor\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWidget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0mWidget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_widget_constructed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/traitlets/traitlets.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1355\u001b[0m             \u001b[0mchanged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchanged\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1358\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m                 \u001b[0mchanges\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'new'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/traitlets/traitlets.py\u001b[0m in \u001b[0;36m_cross_validate\u001b[0;34m(self, obj, value)\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trait_validators\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0mproposal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"trait\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"value\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"owner\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trait_validators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_%s_validate\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m             \u001b[0mmeth_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"_%s_validate\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/traitlets/traitlets.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0;34m\"\"\"Pass `*args` and `**kwargs` to the handler's function if it exists.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"func\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipywidgets/widgets/widget_int.py\u001b[0m in \u001b[0;36m_validate_max\u001b[0;34m(self, proposal)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproposal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTraitError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'setting max < min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTraitError\u001b[0m: setting max < min"
     ]
    }
   ],
   "source": [
    "def show_raw(frame_idx=0):\n",
    "    img = sonar_images[frame_idx]\n",
    "    fov_deg = fov_list[frame_idx]; max_r = range_list[frame_idx]\n",
    "    cart, extent = polar_to_cartesian(img, max_r, fov_deg, 400)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    ax[0].imshow(img, cmap='gray', origin='lower', aspect='auto')\n",
    "    ax[0].set_title(f'Polar frame {frame_idx}')\n",
    "    ax[0].set_xlabel('Beam'); ax[0].set_ylabel('Range bin')\n",
    "    ax[1].imshow(cart, cmap='gray', origin='lower', extent=extent, aspect='equal')\n",
    "    ax[1].set_title(f'Cartesian frame {frame_idx}')\n",
    "    ax[1].set_xlabel('X (m)'); ax[1].set_ylabel('Y (m)')\n",
    "    ax[1].grid(True, alpha=0.3)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "interact(show_raw, frame_idx=IntSlider(min=0, max=len(sonar_images)-1, value=0, step=1, description='Frame'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f599c8",
   "metadata": {},
   "source": [
    "## 4. Net extraction in Cartesian (edges/lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2590d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e50e2bacef4e17a22f86bec6a9c597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Frame', max=999), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_net(frame_idx=0):\n",
    "    img = sonar_images[frame_idx]\n",
    "    fov_deg = fov_list[frame_idx]\n",
    "    max_r = range_list[frame_idx]\n",
    "\n",
    "    img_u8 = (img * 255).astype(np.uint8)\n",
    "    den_img = denoise(img_u8)\n",
    "    enh_img = enhance(den_img)\n",
    "    edge_img = edges(enh_img)\n",
    "    line_segments = lines(edge_img)\n",
    "\n",
    "    cart_raw, extent = polar_to_cartesian(img, max_r, fov_deg, 400)\n",
    "    cart_edges, _ = polar_to_cartesian(edge_img.astype(np.float32) / 255.0, max_r, fov_deg, 400)\n",
    "\n",
    "    # Edge points to Cartesian\n",
    "    r_bins, n_beams = img.shape\n",
    "    fov_rad = np.radians(fov_deg)\n",
    "    pts = np.argwhere(edge_img > 0)  # (row=r, col=beam)\n",
    "    cart_pts = None\n",
    "    if len(pts) > 0:\n",
    "        r = (pts[:, 0] / (r_bins - 1)) * max_r\n",
    "        th = ((pts[:, 1] / (n_beams - 1)) - 0.5) * fov_rad\n",
    "        cx = r * np.sin(th)\n",
    "        cy = r * np.cos(th)\n",
    "        cart_pts = np.stack([cx, cy], axis=1)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    ax[0, 0].imshow(cart_raw, cmap='gray', origin='lower', extent=extent, aspect='equal')\n",
    "    ax[0, 0].set_title('Cartesian raw')\n",
    "\n",
    "    ax[0, 1].imshow(cart_edges, cmap='gray', origin='lower', extent=extent, aspect='equal')\n",
    "    ax[0, 1].set_title(f'Edges (Canny)')\n",
    "\n",
    "    # Lines overlay\n",
    "    ax[1, 0].imshow(cart_raw, cmap='gray', origin='lower', extent=extent, aspect='equal', alpha=0.8)\n",
    "    if line_segments is not None and len(line_segments) > 0:\n",
    "        for seg in line_segments:\n",
    "            x1, y1, x2, y2 = seg[0]\n",
    "            r1 = (y1 / (r_bins - 1)) * max_r; r2 = (y2 / (r_bins - 1)) * max_r\n",
    "            t1 = ((x1 / (n_beams - 1)) - 0.5) * fov_rad; t2 = ((x2 / (n_beams - 1)) - 0.5) * fov_rad\n",
    "            cx1, cy1 = r1 * np.sin(t1), r1 * np.cos(t1)\n",
    "            cx2, cy2 = r2 * np.sin(t2), r2 * np.cos(t2)\n",
    "            ax[1, 0].plot([cx1, cx2], [cy1, cy2], color='yellow', linewidth=2, alpha=0.8)\n",
    "    ax[1, 0].set_title(f'Lines (Hough)')\n",
    "\n",
    "    # Edge points scatter\n",
    "    ax[1, 1].imshow(cart_raw, cmap='gray', origin='lower', extent=extent, aspect='equal', alpha=0.5)\n",
    "    if cart_pts is not None:\n",
    "        ax[1, 1].scatter(cart_pts[:, 0], cart_pts[:, 1], s=4, c='cyan', alpha=0.6)\n",
    "    ax[1, 1].set_title(f'Edge points ({0 if cart_pts is None else len(cart_pts)})')\n",
    "\n",
    "    for a in ax.flat:\n",
    "        a.grid(True, alpha=0.3)\n",
    "        a.set_xlabel('X (m)')\n",
    "        a.set_ylabel('Y (m)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "interact(extract_net, frame_idx=IntSlider(min=0, max=len(sonar_images)-1, value=0, step=1, description='Frame'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94e3738",
   "metadata": {},
   "source": [
    "## 5. Ground Truth for ML: Semantic Segmentation\n",
    "\n",
    "**Why Segmentation beats Bounding Boxes:**\n",
    "- ✅ Nets curve, bend, and have irregular shapes\n",
    "- ✅ Pixel-wise labels capture exact net geometry  \n",
    "- ✅ No wasted space including fish/debris\n",
    "- ✅ Works for partial/occluded nets\n",
    "\n",
    "**Recommended Approach: Binary Segmentation**\n",
    "- **Input**: Sonar image (1024×256 or 400×400 Cartesian)\n",
    "- **Output**: Binary mask (net=1, background=0)\n",
    "- **Architecture**: U-Net, SegFormer, or DeepLabV3+\n",
    "- **Loss**: Dice Loss + BCE (handles class imbalance)\n",
    "\n",
    "**Data Pipeline:**\n",
    "1. Extract ground truth material IDs from simulation\n",
    "2. Convert net material → binary mask\n",
    "3. Apply polar-to-Cartesian transform to both image & mask\n",
    "4. Train segmentation model\n",
    "5. Post-process predictions with line fitting/thinning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ac0ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4335e00673e34a688fb08137a557466c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Frame', max=999), IntSlider(value=1, description='Mat ID…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_segmentation_mask(frame_idx=0, net_material_id=1, thickness=3):\n",
    "    \"\"\"\n",
    "    Create binary segmentation mask from ground truth material IDs.\n",
    "    \n",
    "    Material IDs from simulation (simulation/src/core/materials.py):\n",
    "        0 = EMPTY (water/air)\n",
    "        1 = NET (fishing nets) ← DEFAULT\n",
    "        2 = ROPE (support ropes)\n",
    "        3 = FISH (fish bodies)\n",
    "        4 = WALL (solid barriers)\n",
    "        5 = BIOMASS (organic accumulation)\n",
    "        6-8 = DEBRIS (light/medium/heavy)\n",
    "        9 = CONCRETE, 10 = WOOD, 11 = FOLIAGE, 12 = METAL, 13 = GLASS\n",
    "    \n",
    "    Args:\n",
    "        frame_idx: Frame index\n",
    "        net_material_id: Material ID to extract (default 1 = NET)\n",
    "        thickness: Line thickness in pixels for visualization/training\n",
    "    \n",
    "    Returns:\n",
    "        mask_polar, mask_cartesian, img_cartesian, extent\n",
    "    \"\"\"\n",
    "    # Load ground truth\n",
    "    frame_path = frame_files[frame_idx]\n",
    "    data = np.load(frame_path)\n",
    "    \n",
    "    if 'ground_truth' not in data:\n",
    "        print(\"No ground truth available\")\n",
    "        return None, None\n",
    "    \n",
    "    ground_truth = data['ground_truth']  # Material ID map (1024, 256)\n",
    "    sonar_img = data['sonar_image']\n",
    "    \n",
    "    # Create binary mask: net=1, everything else=0\n",
    "    mask_polar = (ground_truth == net_material_id).astype(np.uint8) * 255\n",
    "    \n",
    "    # Optionally thicken lines for better training\n",
    "    if thickness > 1:\n",
    "        kernel = np.ones((thickness, thickness), np.uint8)\n",
    "        mask_polar = cv2.dilate(mask_polar, kernel, iterations=1)\n",
    "    \n",
    "    # Convert to Cartesian\n",
    "    fov_deg = fov_list[frame_idx]\n",
    "    max_r = range_list[frame_idx]\n",
    "    mask_cart, extent = polar_to_cartesian(mask_polar.astype(np.float32) / 255.0, max_r, fov_deg, 400)\n",
    "    mask_cart = (mask_cart > 0.5).astype(np.uint8) * 255  # Re-binarize after interpolation\n",
    "    \n",
    "    # Also get image\n",
    "    img_cart, _ = polar_to_cartesian(sonar_img, max_r, fov_deg, 400)\n",
    "    \n",
    "    data.close()\n",
    "    \n",
    "    return mask_polar, mask_cart, img_cart, extent\n",
    "\n",
    "\n",
    "def visualize_segmentation_gt(frame_idx=0, net_material_id=1, thickness=2):\n",
    "    \"\"\"\n",
    "    Visualize sonar image + ground truth mask overlay.\n",
    "    \n",
    "    Use the Mat ID slider to explore different materials:\n",
    "    - 1 = NET (default - should show net structure)\n",
    "    - 2 = ROPE (net support cables)\n",
    "    - 3 = FISH (individual fish)\n",
    "    - 5 = BIOMASS (accumulated organic matter)\n",
    "    \"\"\"\n",
    "    result = create_segmentation_mask(frame_idx, net_material_id, thickness)\n",
    "    \n",
    "    if result[0] is None:\n",
    "        print(\"No ground truth available for this frame\")\n",
    "        return\n",
    "    \n",
    "    mask_polar, mask_cart, img_cart, extent = result\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    \n",
    "    # Raw image\n",
    "    ax[0].imshow(img_cart, cmap='gray', origin='lower', extent=extent, aspect='equal')\n",
    "    ax[0].set_title('Sonar Image (Input)')\n",
    "    ax[0].grid(True, alpha=0.3)\n",
    "    ax[0].set_xlabel('X (m)'); ax[0].set_ylabel('Y (m)')\n",
    "    \n",
    "    # Ground truth mask\n",
    "    ax[1].imshow(mask_cart, cmap='hot', origin='lower', extent=extent, aspect='equal')\n",
    "    ax[1].set_title(f'Ground Truth Mask (Mat ID={net_material_id})')\n",
    "    ax[1].grid(True, alpha=0.3)\n",
    "    ax[1].set_xlabel('X (m)'); ax[1].set_ylabel('Y (m)')\n",
    "    \n",
    "    # Overlay\n",
    "    ax[2].imshow(img_cart, cmap='gray', origin='lower', extent=extent, aspect='equal', alpha=0.8)\n",
    "    ax[2].imshow(mask_cart, cmap='hot', origin='lower', extent=extent, aspect='equal', alpha=0.4)\n",
    "    ax[2].set_title('Overlay (Image + Mask)')\n",
    "    ax[2].grid(True, alpha=0.3)\n",
    "    ax[2].set_xlabel('X (m)'); ax[2].set_ylabel('Y (m)')\n",
    "    \n",
    "    # Stats\n",
    "    net_pixels = np.sum(mask_cart > 0)\n",
    "    total_pixels = mask_cart.size\n",
    "    coverage = 100 * net_pixels / total_pixels\n",
    "    print(f\"Material ID {net_material_id}: {net_pixels}/{total_pixels} pixels ({coverage:.2f}% coverage)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "interact(visualize_segmentation_gt, \n",
    "         frame_idx=IntSlider(min=0, max=len(sonar_images)-1, value=0, step=1, description='Frame'),\n",
    "         net_material_id=IntSlider(min=1, max=13, value=1, step=1, description='Mat ID'),\n",
    "         thickness=IntSlider(min=1, max=5, value=2, step=1, description='Thickness'));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1af338",
   "metadata": {},
   "source": [
    "## 6. ML Pipeline: Step 1 - Export Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63928f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1000 frames (800 train, 200 val)...\n",
      "Material ID: 1, Thickness: 2px\n",
      "  Processed 100/1000 frames...\n",
      "  Processed 200/1000 frames...\n",
      "  Processed 300/1000 frames...\n",
      "  Processed 400/1000 frames...\n",
      "  Processed 500/1000 frames...\n",
      "  Processed 600/1000 frames...\n",
      "  Processed 700/1000 frames...\n",
      "  Processed 800/1000 frames...\n",
      "  Processed 900/1000 frames...\n",
      "  Processed 1000/1000 frames...\n",
      "\n",
      "✅ Dataset exported to: /Users/eirikvarnes/code/sonar-sim/net_localization/../training_data\n",
      "   Train: 800 image-mask pairs\n",
      "   Val: 200 image-mask pairs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('../training_data')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def export_training_data(output_dir='../training_data', net_material_id=1, thickness=2, \n",
    "                         train_split=0.8, num_frames=None):\n",
    "    \"\"\"\n",
    "    Export training dataset as PNG image pairs.\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Directory to save data\n",
    "        net_material_id: Material ID to extract (1=NET)\n",
    "        thickness: Mask thickness in pixels\n",
    "        train_split: Fraction for training (rest is validation)\n",
    "        num_frames: Number of frames to export (None = all)\n",
    "    \n",
    "    Creates:\n",
    "        output_dir/\n",
    "            train/\n",
    "                images/000000.png, 000001.png, ...\n",
    "                masks/000000.png, 000001.png, ...\n",
    "            val/\n",
    "                images/000800.png, 000801.png, ...\n",
    "                masks/000800.png, 000801.png, ...\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    from PIL import Image\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    # Create directories\n",
    "    for split in ['train', 'val']:\n",
    "        (output_path / split / 'images').mkdir(parents=True, exist_ok=True)\n",
    "        (output_path / split / 'masks').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Determine frames to process\n",
    "    total_frames = len(frame_files) if num_frames is None else min(num_frames, len(frame_files))\n",
    "    train_count = int(total_frames * train_split)\n",
    "    \n",
    "    print(f\"Exporting {total_frames} frames ({train_count} train, {total_frames - train_count} val)...\")\n",
    "    print(f\"Material ID: {net_material_id}, Thickness: {thickness}px\")\n",
    "    \n",
    "    for i in range(total_frames):\n",
    "        # Generate mask\n",
    "        result = create_segmentation_mask(i, net_material_id, thickness)\n",
    "        if result[0] is None:\n",
    "            print(f\"Skipping frame {i}: no ground truth\")\n",
    "            continue\n",
    "        \n",
    "        mask_polar, mask_cart, img_cart, extent = result\n",
    "        \n",
    "        # Normalize image to 0-255\n",
    "        img_norm = ((img_cart - img_cart.min()) / (img_cart.max() - img_cart.min() + 1e-8) * 255).astype(np.uint8)\n",
    "        \n",
    "        # Determine split\n",
    "        split = 'train' if i < train_count else 'val'\n",
    "        \n",
    "        # Save as PNG\n",
    "        img_pil = Image.fromarray(img_norm)\n",
    "        mask_pil = Image.fromarray(mask_cart)\n",
    "        \n",
    "        img_pil.save(output_path / split / 'images' / f'{i:06d}.png')\n",
    "        mask_pil.save(output_path / split / 'masks' / f'{i:06d}.png')\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  Processed {i + 1}/{total_frames} frames...\")\n",
    "    \n",
    "    print(f\"\\n✅ Dataset exported to: {output_path.absolute()}\")\n",
    "    print(f\"   Train: {train_count} image-mask pairs\")\n",
    "    print(f\"   Val: {total_frames - train_count} image-mask pairs\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Export dataset (uncomment to run)\n",
    "export_training_data(num_frames=1000, train_split=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43c1459",
   "metadata": {},
   "source": [
    "## 7. ML Pipeline: Step 2 - Install Dependencies\n",
    "\n",
    "**For Google Colab:** These will install on the cloud instance.  \n",
    "**For local:** Skip if already installed with `pip install -r requirements_ml.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a900851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dependencies ready (uncomment above to install)\n"
     ]
    }
   ],
   "source": [
    "# Install ML dependencies (uncomment to run)\n",
    "# !pip install torch torchvision segmentation-models-pytorch albumentations tqdm\n",
    "\n",
    "print(\"✅ Dependencies ready (uncomment above to install)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812547b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training code template prepared. Save as 'train_segmentation.py' to use.\n",
      "\n",
      "Quick start:\n",
      "  1. Export data: export_training_data(num_frames=1000)\n",
      "  2. Install deps: pip install torch segmentation-models-pytorch albumentations\n",
      "  3. Train: python train_segmentation.py --epochs 50\n"
     ]
    }
   ],
   "source": [
    "## 8. ML Pipeline: Step 3 - Dataset & Training Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ff78c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import segmentation_models_pytorch as smp\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "class SonarDataset(Dataset):\n",
    "    \"\"\"Load sonar images and masks from PNG files\"\"\"\n",
    "    def __init__(self, data_dir, split='train', transform=None):\n",
    "        self.img_dir = Path(data_dir) / split / 'images'\n",
    "        self.mask_dir = Path(data_dir) / split / 'masks'\n",
    "        self.files = sorted(self.img_dir.glob('*.png'))\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.files[idx]\n",
    "        mask_path = self.mask_dir / img_path.name\n",
    "        \n",
    "        # Load as grayscale\n",
    "        image = np.array(Image.open(img_path))\n",
    "        mask = np.array(Image.open(mask_path))\n",
    "        \n",
    "        # Convert to 3-channel (required by pretrained models)\n",
    "        if len(image.shape) == 2:\n",
    "            image = np.stack([image, image, image], axis=-1)\n",
    "        \n",
    "        # Binarize mask\n",
    "        mask = (mask > 127).astype(np.float32)\n",
    "        \n",
    "        # Apply augmentations\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "        \n",
    "        return image, mask.unsqueeze(0)  # Add channel dim\n",
    "\n",
    "\n",
    "def get_transforms(train=True):\n",
    "    \"\"\"Data augmentation for sonar images\"\"\"\n",
    "    if train:\n",
    "        return A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Rotate(limit=15, p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.3),\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Dice Loss for handling class imbalance\"\"\"\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred = torch.sigmoid(pred)\n",
    "        intersection = (pred * target).sum(dim=(2, 3))\n",
    "        union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "\n",
    "print(\"✅ Dataset and loss classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e80186",
   "metadata": {},
   "source": [
    "## 9. ML Pipeline: Step 4 - Train Model\n",
    "\n",
    "**Training Parameters:**\n",
    "- `epochs`: Number of training epochs (start with 20-50)\n",
    "- `batch_size`: Batch size (8 for GPU, 2-4 for CPU)\n",
    "- `lr`: Learning rate (1e-4 is good default)\n",
    "\n",
    "**Google Colab:** Make sure to enable GPU runtime (Runtime → Change runtime type → GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48782aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_dir='training_data', epochs=20, batch_size=8, lr=1e-4):\n",
    "    \"\"\"Train U-Net segmentation model\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Validate data directory\n",
    "    data_path = Path(data_dir)\n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(f\"Data directory not found: {data_dir}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = SonarDataset(data_dir, 'train', get_transforms(train=True))\n",
    "    val_dataset = SonarDataset(data_dir, 'val', get_transforms(train=False))\n",
    "    \n",
    "    # Use num_workers=0 for compatibility (Colab/macOS)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"Train: {len(train_dataset)} images, Val: {len(val_dataset)} images\")\n",
    "    \n",
    "    # Create model (U-Net with ResNet34 encoder)\n",
    "    model = smp.Unet(\n",
    "        encoder_name='resnet34',\n",
    "        encoder_weights='imagenet',\n",
    "        in_channels=3,\n",
    "        classes=1,\n",
    "        activation=None  # We'll apply sigmoid in loss\n",
    "    ).to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = DiceLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, masks in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_net_segmentation.pth')\n",
    "            print(f\"  ✅ Saved best model (val_loss={val_loss:.4f})\")\n",
    "    \n",
    "    print(\"\\n✅ Training complete!\")\n",
    "    return model\n",
    "\n",
    "# Train the model (uncomment to run)\n",
    "# trained_model = train_model(data_dir='training_data', epochs=20, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f889abc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(model_path='best_net_segmentation.pth'):\n",
    "    \"\"\"Load trained segmentation model\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = smp.Unet(\n",
    "        encoder_name='resnet34',\n",
    "        encoder_weights=None,\n",
    "        in_channels=3,\n",
    "        classes=1,\n",
    "        activation='sigmoid'\n",
    "    ).to(device)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"✅ Model loaded from {model_path} on {device}\")\n",
    "    return model, device\n",
    "\n",
    "\n",
    "def predict_net_mask(model, device, image_cart):\n",
    "    \"\"\"\n",
    "    Run inference on Cartesian sonar image.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained segmentation model\n",
    "        device: torch device\n",
    "        image_cart: Cartesian sonar image (H, W)\n",
    "    \n",
    "    Returns:\n",
    "        Binary mask (H, W) with net predictions\n",
    "    \"\"\"\n",
    "    # Prepare image\n",
    "    img_norm = ((image_cart - image_cart.min()) / (image_cart.max() - image_cart.min() + 1e-8) * 255).astype(np.uint8)\n",
    "    img_3ch = np.stack([img_norm, img_norm, img_norm], axis=-1)\n",
    "    \n",
    "    # Transform\n",
    "    transform = A.Compose([\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    img_tensor = transform(image=img_3ch)['image'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        mask_pred = (output[0, 0].cpu().numpy() > 0.5).astype(np.uint8) * 255\n",
    "    \n",
    "    return mask_pred\n",
    "\n",
    "# Load model (uncomment after training)\n",
    "# model, device = load_trained_model('best_net_segmentation.pth')\n",
    "print(\"Model loading functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbc0682",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11. ML Pipeline: Step 6 - Visualize Results with Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28b67b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_net_lines_from_mask(mask, min_length=20):\n",
    "    \"\"\"\n",
    "    Convert binary mask to line segments.\n",
    "    \n",
    "    This is the key post-processing step that converts ML predictions\n",
    "    back to geometric representations (lines) for tracking/control.\n",
    "    \n",
    "    Args:\n",
    "        mask: Binary mask (255 = net, 0 = background)\n",
    "        min_length: Minimum line length in pixels\n",
    "    \n",
    "    Returns:\n",
    "        lines: Array of line segments [(x1,y1,x2,y2), ...]\n",
    "    \"\"\"\n",
    "    # 1. Thin the mask to skeleton (1-pixel wide)\n",
    "    mask_binary = (mask > 127).astype(np.uint8)\n",
    "    skeleton = morphological_skeleton(mask_binary)\n",
    "    skeleton_u8 = (skeleton * 255).astype(np.uint8)\n",
    "    \n",
    "    # 2. Find contours\n",
    "    contours, _ = cv2.findContours(skeleton_u8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # 3. Fit lines to contours\n",
    "    lines = []\n",
    "    for contour in contours:\n",
    "        if len(contour) < 5:  # Need at least 5 points for fitLine\n",
    "            continue\n",
    "        \n",
    "        # Fit line using least squares\n",
    "        [vx, vy, x, y] = cv2.fitLine(contour, cv2.DIST_L2, 0, 0.01, 0.01)\n",
    "        \n",
    "        # Get endpoints from contour extent\n",
    "        pts = contour.reshape(-1, 2)\n",
    "        x_min, y_min = pts.min(axis=0)\n",
    "        x_max, y_max = pts.max(axis=0)\n",
    "        \n",
    "        # Project endpoints onto fitted line\n",
    "        length = np.sqrt((x_max - x_min)**2 + (y_max - y_min)**2)\n",
    "        \n",
    "        if length < min_length:\n",
    "            continue\n",
    "        \n",
    "        # Compute line endpoints\n",
    "        t1 = -length / 2\n",
    "        t2 = length / 2\n",
    "        x1, y1 = x + vx * t1, y + vy * t1\n",
    "        x2, y2 = x + vx * t2, y + vy * t2\n",
    "        \n",
    "        lines.append([x1[0], y1[0], x2[0], y2[0]])\n",
    "    \n",
    "    return np.array(lines) if lines else np.array([]).reshape(0, 4)\n",
    "\n",
    "\n",
    "def visualize_ml_pipeline(frame_idx=0, model=None, device=None, show_gt=True):\n",
    "    \"\"\"\n",
    "    Visualize complete ML pipeline: Input → Prediction → Lines.\n",
    "    \n",
    "    Args:\n",
    "        frame_idx: Frame index\n",
    "        model: Trained model (None = use ground truth as proxy)\n",
    "        device: torch device\n",
    "        show_gt: Show ground truth comparison\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    result = create_segmentation_mask(frame_idx, net_material_id=1, thickness=2)\n",
    "    if result[0] is None:\n",
    "        print(\"No data for this frame\")\n",
    "        return\n",
    "    \n",
    "    mask_polar, mask_gt, img_cart, extent = result\n",
    "    \n",
    "    # Predict (or use ground truth as proxy)\n",
    "    if model is not None:\n",
    "        mask_pred = predict_net_mask(model, device, img_cart)\n",
    "        print(f\"✅ Running TRAINED model prediction\")\n",
    "    else:\n",
    "        mask_pred = mask_gt  # Use ground truth as \"prediction\" for demo\n",
    "        print(\"⚠️  Using ground truth as proxy (train model first)\")\n",
    "    \n",
    "    # Extract lines from prediction\n",
    "    lines = extract_net_lines_from_mask(mask_pred, min_length=20)\n",
    "    \n",
    "    # Visualize\n",
    "    n_cols = 4 if show_gt else 3\n",
    "    fig, ax = plt.subplots(1, n_cols, figsize=(6*n_cols, 6))\n",
    "    \n",
    "    # 1. Input image\n",
    "    ax[0].imshow(img_cart, cmap='gray', origin='lower', extent=extent, aspect='equal')\n",
    "    ax[0].set_title('Input: Sonar Image')\n",
    "    ax[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Predicted mask\n",
    "    ax[1].imshow(img_cart, cmap='gray', origin='lower', extent=extent, aspect='equal', alpha=0.5)\n",
    "    ax[1].imshow(mask_pred, cmap='hot', origin='lower', extent=extent, aspect='equal', alpha=0.5)\n",
    "    title = 'ML Prediction' if model is not None else 'Ground Truth (Proxy)'\n",
    "    ax[1].set_title(title)\n",
    "    ax[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Extracted lines\n",
    "    ax[2].imshow(img_cart, cmap='gray', origin='lower', extent=extent, aspect='equal', alpha=0.5)\n",
    "    if len(lines) > 0:\n",
    "        # Convert pixel coords to meters\n",
    "        h, w = img_cart.shape\n",
    "        x_min, x_max, y_min, y_max = extent\n",
    "        for line in lines:\n",
    "            x1_m = x_min + (line[0] / w) * (x_max - x_min)\n",
    "            y1_m = y_min + (line[1] / h) * (y_max - y_min)\n",
    "            x2_m = x_min + (line[2] / w) * (x_max - x_min)\n",
    "            y2_m = y_min + (line[3] / h) * (y_max - y_min)\n",
    "            ax[2].plot([x1_m, x2_m], [y1_m, y2_m], 'lime', linewidth=2, alpha=0.8)\n",
    "    ax[2].set_title(f'Output: Line Segments ({len(lines)} lines)')\n",
    "    ax[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Ground truth comparison (optional)\n",
    "    if show_gt:\n",
    "        ax[3].imshow(img_cart, cmap='gray', origin='lower', extent=extent, aspect='equal', alpha=0.5)\n",
    "        ax[3].imshow(mask_gt, cmap='spring', origin='lower', extent=extent, aspect='equal', alpha=0.5)\n",
    "        ax[3].set_title('Ground Truth (for comparison)')\n",
    "        ax[3].grid(True, alpha=0.3)\n",
    "    \n",
    "    for a in ax:\n",
    "        a.set_xlabel('X (m)')\n",
    "        a.set_ylabel('Y (m)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✅ Extracted {len(lines)} line segments from mask\")\n",
    "    return lines\n",
    "\n",
    "# Demo with ground truth as proxy (before training)\n",
    "interact(visualize_ml_pipeline, \n",
    "         frame_idx=IntSlider(min=0, max=len(sonar_images)-1, value=0, description='Frame'),\n",
    "         model=fixed(None),\n",
    "         device=fixed(None),\n",
    "         show_gt=fixed(True));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caed6fe",
   "metadata": {},
   "source": [
    "## 12. Test Trained Model (Run After Training)\n",
    "\n",
    "After training is complete, use this cell to load the model and test predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a1e9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model and visualize predictions (uncomment after training)\n",
    "# model, device = load_trained_model('best_net_segmentation.pth')\n",
    "\n",
    "# interact(visualize_ml_pipeline, \n",
    "#          frame_idx=IntSlider(min=0, max=len(sonar_images)-1, value=0, description='Frame'),\n",
    "#          model=fixed(model),\n",
    "#          device=fixed(device),\n",
    "#          show_gt=fixed(True));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
